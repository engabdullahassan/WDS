{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "FOR MAC: tensorflow-metal\n",
    "\n",
    "\n",
    "Accelerate the training of machine learning models with TensorFlow right on your Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING VIRTUAL ENVIRONMENT\n",
    "\n",
    "#make sure kerranl and ternimal match!\n",
    "\n",
    "#create\n",
    "!python3 -m venv ~/MyEnvo/venv-metal\n",
    "\n",
    "#actiavte\n",
    "!source ~/MyEnvo/venv-metal/bin/activate\n",
    "!python -m pip install -U pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install  protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base tensorfloe for mac os\n",
    "#Tensorflow: libary for machine learning/CNN\n",
    "!python -m pip install tensorflow-macos==2.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To utilize gpu\n",
    "!python -m pip install tensorflow-metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verififaction of mac gpu\n",
    "# can also check activity monitor\n",
    "import tensorflow as tf\n",
    "\n",
    "cifar = tf.keras.datasets.cifar100\n",
    "(x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "model = tf.keras.applications.ResNet50(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_shape=(32, 32, 3),\n",
    "    classes=100,)\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_hi5MlEI_t1b",
    "outputId": "bfa6619b-8d4d-438c-9086-65956d8344b8"
   },
   "outputs": [],
   "source": [
    "#REDEFINE to use easily on differnt computers, define to path this file is\n",
    "\n",
    "#path for our github repositiory on local computer\n",
    "rep_path='/Users/Aida/Documents/GitHub/Rep2' \n",
    "\n",
    "#Check if path correct & working in this path\n",
    "%cd $rep_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python >=3.8 for tensorflow 2\n",
    "!python --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5eW3Reg-__i0",
    "outputId": "6d402378-5d83-457e-e891-e6b0e9eb2d72"
   },
   "outputs": [],
   "source": [
    "# defining dictory of project\n",
    "#Make sure file is created in correct location\n",
    "%cd $rep_path \n",
    " #creating directory for Weapon Dectection Syatem\n",
    "!mkdir WDS\n",
    "#checking path & reassining working directory\n",
    "%cd WDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uMkjvFG_1BM"
   },
   "outputs": [],
   "source": [
    "##ONLY NEED DO DO ONCE PER COMPUTER/ENVIROMENT\n",
    "\n",
    "#pip used to install packges, de facto standard\n",
    "!python get-pip.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instaling packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify install of tf2\n",
    "!python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking tf version\n",
    "\n",
    "#CHANGES HOW WE SET UP TENSOR FLOW\n",
    "!tensorflow --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mrBVXVyn_4q2",
    "outputId": "be540983-3810-46b1-b520-b758d692934c"
   },
   "outputs": [],
   "source": [
    "#library used in building a custom object detector using Tensorflow Object Detection\n",
    "%pip install Cython\n",
    "%pip install pillow\n",
    "%pip install lxml\n",
    "%pip install jupyter\n",
    "%pip install matplotlib\n",
    "%pip install pycocotools\n",
    "%pip install opencv-python #for cv2: computer vision\n",
    "%pip install pandas\n",
    "%pip install contextlib2\n",
    "%pip install typing_extensions\n",
    "%pip update typing-extensions\n",
    "\n",
    "# pycocotools is a Python API that. # assists in loading, parsing and visualizing the annotations in COCO.\n",
    "%pip install -qq pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!protoc --version  #compiler version>=3 for tf 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERAL IMPORTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQH-c27G_jmB"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Will change the / operator to mean true division throughout the module.\n",
    "# A command line option will enable run-time warnings for classic division applied to int or long arguments; \n",
    "#another command line option will make true division the default.\n",
    "from __future__ import division\n",
    "\n",
    "# Stop package-internal modules from shadowing top-level modules\n",
    "# If you run the file as part of the pkg package, the package's internal files stop showing up as top-level.\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# creates table-like custom objects from the items in CSV files\n",
    "import csv\n",
    "\n",
    "#  A regular expression (or RE) specifies a set of strings that matches it; the functions in this module \n",
    "# let you check if a particular string matches a given regular expression (or if a given regular expression \n",
    "# matches a particular string, which comes down to the same thing)\n",
    "import re\n",
    "\n",
    "# Unofficial pre-built CPU-only OpenCV packages for Python\n",
    "import cv2\n",
    "\n",
    "#lets the user interact with the native OS Python is currently running on\n",
    "import os\n",
    "\n",
    "#search CSV files and for text in files.\n",
    "import glob\n",
    "\n",
    "# his function takes an XML data string (xml_data) or a file path or file-like object (from_file) as input, \n",
    "# converts it to the canonical form, and writes it out using the out file(-like) object, if provided, or returns \n",
    "# it as a text string if not\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "#The io module provides Python’s main facilities for dealing with various types of I/O.\n",
    "#There are three main types of I/O: text I/O, binary I/O and raw I/O.\n",
    "import io\n",
    "\n",
    "import tensorflow as tf \n",
    "\n",
    "# Creates a copy of an image memory from pixel data in a buffer\n",
    "# In its simplest form, this function takes three arguments (mode, size, and unpacked pixel data).\n",
    "from PIL import Image\n",
    "\n",
    "# import namedtuple; namedtuple() is a factory function available in collections \n",
    "# It allows you to create tuple subclasses with named fields\n",
    "#You can access the values in a given named tuple using the dot notation and the field names, like in obj\n",
    "\n",
    "#import OrderedDict; An OrderedDict is a data type in the collections module.\n",
    "# It is a dict subclass that tracks the order in which items were added. \n",
    "from collections import namedtuple, OrderedDict\n",
    "\n",
    "#support file copying and removal.\n",
    "import shutil\n",
    "\n",
    "#defines functions and classes which help in opening URLs (mostly HTTP) in a complex world — \n",
    "# basic and digest authentication, redirections, cookies and more\n",
    "import urllib.request\n",
    "\n",
    "# read and write tar archives\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orgniazing database/ DONT RUN UNLESS MANULY RESET LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on dataset:\n",
    "https://dasci.es/transferencia/open-data/24705/\n",
    "\n",
    "Using pistol and knife detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making sure in right place\n",
    "%cd $rep_path/WDS \n",
    "\n",
    "#creating data directory\n",
    "!mkdir data\n",
    "#creating sub directories\n",
    "!mkdir data/images data/train_labels data/test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pistol and knife images in WDS/data/images\n",
    "\n",
    "https://github.com/ari-dasci/OD-WeaponDetection/trunk/Pistol%20detection/Weapons/\n",
    "\n",
    "https://github.com/ari-dasci/OD-WeaponDetection/tree/master/Knife_detection/Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEEFORE SPLIT PUT ALL THESE FILES WDS/data/train_labels\n",
    "\n",
    "#pistol annaotions/bounding box\n",
    "https://github.com/ari-dasci/OD-WeaponDetection/tree/master/Pistol%20detection/xmls\n",
    "\n",
    "#knife annaotions/bounding box\n",
    "https://github.com/ari-dasci/OD-WeaponDetection/tree/master/Knife_detection/annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONT RERUN!!!!\n",
    "#WARNING!!!!!!!!!!! DONT RUN WITHOUT HAVING ALL LABELS IN TRAIN LABELS FIRST AND CLEARING TEST FOLDER\n",
    "\n",
    "#spliting data 75%train 25%test\n",
    "#3000 pistol+2078knife=5078\n",
    "#25% => 1270 rounded\n",
    "\n",
    "#spliting data 85%train 15%test\n",
    "#3000 pistol+2078knife=5078\n",
    "#15% => 762 rounded\n",
    "\n",
    "#ASSUMING anotations are initially in train labels\n",
    "#start with train as most data will be in trian\n",
    "#random slipt of data\n",
    "%cd $rep_path/WDS\n",
    "\n",
    "!ls $rep_path/WDS/data/train_labels/* | sort -R | head -762 | xargs -I{} mv {} $rep_path/WDS/data/test_labels\n",
    "#!ls $rep_path/WDS/data/train_labels/* | sort -R | head -1270 | xargs -I{} mv {} $rep_path/WDS/data/test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking if we have right ammount of labels for traning and test\n",
    "!ls -1 $rep_path/WDS/data/train_labels/ | wc -l\n",
    "!ls -1 $rep_path/WDS/data/test_labels/ | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preproocesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runing in right place\n",
    "%cd $rep_path/WDS/data\n",
    "\n",
    "# images extension\n",
    "images_extension = 'jpg'\n",
    "\n",
    "# takes the path of a directory that contains xml files and converts\n",
    "#  them to one csv file.\n",
    "\n",
    "# returns a csv file that contains: image name, width, height, class, xmin, ymin, xmax, ymax.\n",
    "# note: if the xml file contains more than one box/label, it will create more than one row for the same image. each row contains the info for an individual box. \n",
    "def xml_to_csv(path):\n",
    "  classes_names = []\n",
    "  xml_list = []\n",
    "\n",
    "  for xml_file in glob.glob(path + '/*.xml'):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    for member in root.findall('object'):\n",
    "      classes_names.append(member[0].text)\n",
    "      #classes_names.append(member[1].text)\n",
    "      #fixing double .jpg error wth knife\n",
    "      aString=root.find('filename').text\n",
    "      if aString.startswith(\"armas\"):\n",
    "        value = (root.find('filename').text + '.' + images_extension,\n",
    "               int(root.find('size')[0].text),\n",
    "               int(root.find('size')[1].text),\n",
    "               member[0].text,\n",
    "               int(member[4][0].text),\n",
    "               int(member[4][1].text),\n",
    "               int(member[4][2].text),\n",
    "               int(member[4][3].text))\n",
    "      else:\n",
    "        value = (root.find('filename').text,\n",
    "               int(root.find('size')[0].text),\n",
    "               int(root.find('size')[1].text),\n",
    "               member[0].text,\n",
    "               int(member[4][0].text),\n",
    "               int(member[4][1].text),\n",
    "               int(member[4][2].text),\n",
    "               int(member[4][3].text))\n",
    "      xml_list.append(value)\n",
    "  column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
    "  xml_df = pd.DataFrame(xml_list, columns=column_name) \n",
    "  classes_names = list(set(classes_names))\n",
    "  classes_names.sort()\n",
    "  return xml_df, classes_names\n",
    "\n",
    "# for both the train_labels and test_labels csv files, it runs the xml_to_csv() above.\n",
    "for label_path in ['train_labels', 'test_labels']:\n",
    "  image_path = os.path.join(os.getcwd(), label_path)\n",
    "  xml_df, classes = xml_to_csv(label_path)\n",
    "  xml_df.to_csv(f'{label_path}.csv', index=None)\n",
    "  print(f'Successfully converted {label_path} xml to csv.')\n",
    "\n",
    "# Creating the `label_map.pbtxt` file\n",
    "label_map_path = os.path.join(\"label_map.pbtxt\")\n",
    "\n",
    "pbtxt_content = \"\"\n",
    "\n",
    "#creats a pbtxt file the has the class names.\n",
    "for i, class_name in enumerate(classes):\n",
    "    # display_name is optional.\n",
    "    pbtxt_content = (\n",
    "        pbtxt_content\n",
    "        # + \"item {{\\n    id: {0}\\n    name: '{1}'\\n    display_name: 'Knife'\\n }}\\n\\n\".format(i + 1, class_name)\n",
    "        #+ \"item {{\\n    id: {1}\\n    name: '{2}'\\n    display_name: 'Pistol'\\n }}\\n\\n\".format(i + 1, class_name)\n",
    "        + \"item {{\\n    id: {0}\\n    name: '{1}'\\n  }}\\n\\n\".format(i + 1, class_name)\n",
    "    )\n",
    "pbtxt_content = pbtxt_content.strip()\n",
    "with open(label_map_path, \"w\") as f:\n",
    "    f.write(pbtxt_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the pbtxt file\n",
    "%cd $rep_path/WDS/data\n",
    "\n",
    "!cat label_map.pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# They are there!\n",
    "%cd $rep_path/WDS/data\n",
    "\n",
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checks if the images box position is placed within the image.\n",
    "\n",
    "#note: while this doesn't checks if the boxes/annotatoins are correctly\n",
    "# placed around the object, Tensorflow will through an error if this occured.\n",
    "\n",
    "# path to images\n",
    "images_path=rep_path+'/WDS/data/images/'\n",
    "\n",
    "#loops over both train_labels and test_labels csv files to do the check\n",
    "# returns the image name where an error is found \n",
    "# return the incorrect attributes; xmin, ymin, xmax, ymax.\n",
    "for CSV_FILE in ['train_labels.csv', 'test_labels.csv']:\n",
    "  with open(CSV_FILE, 'r') as fid:  \n",
    "      print('Checking file:', CSV_FILE) \n",
    "      file = csv.reader(fid, delimiter=',')\n",
    "      first = True \n",
    "      cnt = 0\n",
    "      error_cnt = 0\n",
    "      error = False\n",
    "      for row in file:\n",
    "          if error == True:\n",
    "              error_cnt += 1\n",
    "              error = False         \n",
    "          if first == True:\n",
    "              first = False\n",
    "              continue     \n",
    "          cnt += 1      \n",
    "          name, width, height, xmin, ymin, xmax, ymax = row[0], int(row[1]), int(row[2]), int(row[4]), int(row[5]), int(row[6]), int(row[7])     \n",
    "          #path = os.path.join(images_path, name)\n",
    "          path = images_path+ name\n",
    "          img = cv2.imread(path)         \n",
    "          if type(img) == type(None):\n",
    "              error = True\n",
    "              print('Could not read image', img)\n",
    "              continue     \n",
    "          org_height, org_width = img.shape[:2]     \n",
    "          if org_width != width:\n",
    "              error = True\n",
    "              print('Width mismatch for image: ', name, width, '!=', org_width)     \n",
    "          if org_height != height:\n",
    "              error = True\n",
    "              print('Height mismatch for image: ', name, height, '!=', org_height) \n",
    "          if xmin > org_width:\n",
    "              error = True\n",
    "              print('XMIN > org_width for file', name)  \n",
    "          if xmax > org_width:\n",
    "              error = True\n",
    "              print('XMAX > org_width for file', name)\n",
    "          if ymin > org_height:\n",
    "              error = True\n",
    "              print('YMIN > org_height for file', name)\n",
    "          if ymax > org_height:\n",
    "              error = True\n",
    "              print('YMAX > org_height for file', name)\n",
    "          if error == True:\n",
    "              print('Error for file: %s' % name)\n",
    "              print()\n",
    "      print('Checked %d files and realized %d errors' % (cnt, error_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have only one image with incoorect box position, we could just remove it \n",
    "#removing the image \n",
    "%cd $rep_path/WDS/data/\n",
    "!rm images/'armas (2815).jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the xml for that image as well\n",
    "\n",
    "#reading the gun_labels df\n",
    "df = pd.read_csv(rep_path+'/WDS/data/test_labels.csv')\n",
    "# removing armas (2815).jpg\n",
    "df = df[df['filename'] != 'armas (2815).jpg']\n",
    "\n",
    "#reseting the index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#saving the df\n",
    "df.to_csv(rep_path+'/WDS/data/test_labels.csv')\n",
    "\n",
    "\n",
    "df = pd.read_csv(rep_path+'/WDS/data/train_labels.csv')\n",
    "# removing armas (2815).jpg\n",
    "df = df[df['filename'] != 'armas (2815).jpg']\n",
    "\n",
    "#reseting the index\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#saving the df\n",
    "df.to_csv(rep_path+'/WDS/data/train_labels.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FPoJnD1ND-xa"
   },
   "source": [
    "Preparing tensor flow object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking tensor flow version\n",
    "print(tf.__version__)\n",
    "\n",
    "#CHANGES HOW WE SET UP TENSOR FLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l25a4ZO6AEey",
    "outputId": "0379ca1f-82fa-479e-8a5f-3363de4404b1"
   },
   "outputs": [],
   "source": [
    "\n",
    "#API form Model Garden for TensorFlow, needed for object detection API\n",
    "#only need to run once\n",
    "%cd $rep_path/WDS \n",
    "!git clone https://github.com/tensorflow/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for metrics: COCOAPI\n",
    "%cd $rep_path\n",
    "!git clone https://github.com/cocodataset/cocoapi.git\n",
    "%cd cocoapi/PythonAPI\n",
    "!make\n",
    "!cp -r pycocotools $rep_path/WDS/models/research/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $rep_path/WDS/models/research\n",
    "\n",
    "#proto buffer compilation, Protobufs used to conifure modele and trainning parapemetres\n",
    "!protoc object_detection/protos/*.proto --python_out=.\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# You MUST open a new Terminal for the changes in the environment variables to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dg4i8WLxAI8S"
   },
   "outputs": [],
   "source": [
    "%cd $rep_path/WDS/models/research\n",
    "\n",
    "#installing objeect dection api\n",
    "%cp object_detection/packages/tf2/setup.py . #tf2 \n",
    "\n",
    "!python -m pip install .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%cd $rep_path/WDS/models/research\n",
    "\n",
    "#appending directories to system path, run every kernel\n",
    "\n",
    "research_path=rep_path+'/WDS/models/research/'\n",
    "slim_path=rep_path+'/WDS/models/research/slim/'\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(research_path)\n",
    "sys.path.append(slim_path)\n",
    "\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "rClUcAotAGLb",
    "outputId": "f72b6272-3636-4cd2-c456-6eaeebfa2b98"
   },
   "outputs": [],
   "source": [
    "!cd $rep_path/WDS/models/research/slim\n",
    "\n",
    "# Slim is a light wieght & highlvl Tensorflow API: to define, train, & evaluate image classification models \n",
    "#Verify installation of slim\n",
    "!python -c \"from nets import cifarnet; mynet = cifarnet.cifarnet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPBZ3unUAO8R"
   },
   "outputs": [],
   "source": [
    "%cd $rep_path/WDS/models/research/\n",
    "#testing if tensor flow object dection api is corectly installed\n",
    "\n",
    "!python object_detection/builders/model_builder_tf2_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBet8jgQENqP"
   },
   "source": [
    "Making tensor flow records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnvN4OUKAR_4"
   },
   "outputs": [],
   "source": [
    "\n",
    "from object_detection.utils import dataset_util\n",
    "%cd $rep_path/WDS/models/\n",
    "\n",
    "data_path = rep_path +'/WDS/data/'\n",
    "img_path=data_path +'images/'\n",
    "\n",
    "#defing our different classes\n",
    "def class_text_to_int(row_label):\n",
    "\tif row_label == 'knife':\n",
    "\t\treturn 1\n",
    "\tif row_label == 'pistol':\n",
    "\t\treturn 2\n",
    "\telse:\n",
    "\t\tNone\n",
    "\n",
    "#defining slipt\n",
    "def split(df, group):\n",
    "\tdata = namedtuple('data', ['filename', 'object'])\n",
    "\tgb = df.groupby(group)\n",
    "\treturn [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
    "\n",
    "#defining create tf example\n",
    "def create_tf_example(group, path):\n",
    "\twith tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
    "\t\t\tencoded_jpg = fid.read()\n",
    "\tencoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "\timage = Image.open(encoded_jpg_io)\n",
    "\twidth, height = image.size\n",
    "\n",
    "\tfilename = group.filename.encode('utf8')\n",
    "\timage_format = b'jpg'\n",
    "\txmins = []\n",
    "\txmaxs = []\n",
    "\tymins = []\n",
    "\tymaxs = []\n",
    "\tclasses_text = []\n",
    "\tclasses = []\n",
    "\n",
    "\tfor index, row in group.object.iterrows():\n",
    "\t\txmins.append(row['xmin'] / width)\n",
    "\t\txmaxs.append(row['xmax'] / width)\n",
    "\t\tymins.append(row['ymin'] / height)\n",
    "\t\tymaxs.append(row['ymax'] / height)\n",
    "\t\tclasses_text.append(row['class'].encode('utf8'))\n",
    "\t\tclasses.append(class_text_to_int(row['class']))\n",
    "\n",
    "\ttf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "\t\t'image/height': dataset_util.int64_feature(height),\n",
    "\t\t'image/width': dataset_util.int64_feature(width),\n",
    "\t\t'image/filename': dataset_util.bytes_feature(filename),\n",
    "\t\t'image/source_id': dataset_util.bytes_feature(filename),\n",
    "\t\t'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "\t\t'image/format': dataset_util.bytes_feature(image_format),\n",
    "\t\t'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "\t\t'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "\t\t'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "\t\t'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "\t\t'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "\t\t'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "    }))\n",
    "\treturn tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UXyUWmaJAUh6"
   },
   "outputs": [],
   "source": [
    "%cd $rep_path/WDS/models/\n",
    "\n",
    "#creating tf records from CSV\n",
    "\n",
    "for csv in ['train_labels', 'test_labels']:\n",
    "  #training tf record\n",
    "  writer = tf.io.TFRecordWriter(data_path + csv + '.record')\n",
    "  path = os.path.join(img_path)\n",
    "\n",
    "  #cvs to tf record \n",
    "  sample = pd.read_csv(data_path + csv + '.csv')\n",
    "  grouped = split(sample, 'filename')\n",
    "  \n",
    "  #writing records\n",
    "  for group in grouped:\n",
    "    tf_sample = create_tf_example(group, path)\n",
    "    writer.write(tf_sample.SerializeToString())\n",
    "    \n",
    "  writer.close()\n",
    "  out_path = os.path.join(os.getcwd(), data_path + csv + '.record')\n",
    "  print('Finisheed making the TFRecords: {}'.format(data_path +csv + '.record'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XPXlq72zAVhN"
   },
   "outputs": [],
   "source": [
    "#showing whats in data folder\n",
    "%cd $rep_path/WDS/models/\n",
    "!ls -l ../data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For TF2 \n",
    "#Base models used\n",
    "\n",
    "# max Number of training steps.\n",
    "num_steps = 100000\n",
    "# Number of evaluation steps. \n",
    "num_eval_steps = 50\n",
    "\n",
    "selected_model = 'SSD_MobileNet_V2_FPNLite_640x640'\n",
    "\n",
    "\n",
    "# Some models to train on\n",
    "MODELS_CONFIG = {\n",
    "    'SSD_MobileNet_V2_FPNLite_320x320': {\n",
    "        'model_name': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',\n",
    "        'pipeline_file': 'pipeline.config',\n",
    "        'batch_size': 16\n",
    "    },\n",
    "    'EfficientDet_D1_640x640': {\n",
    "        'model_name': 'efficientdet_d1_coco17_tpu-32',\n",
    "        'pipeline_file': 'pipeline.config',\n",
    "        'batch_size': 16\n",
    "    },\n",
    "    'CenterNet_Resnet101_V1_FPN_512x512': {\n",
    "        'model_name': 'centernet_resnet101_v1_fpn_512x512_coco17_tpu-8',\n",
    "        'pipeline_file': 'pipeline.config',\n",
    "        'batch_size': 16\n",
    "    },\n",
    "    'SSD_ResNet101_V1_FPN_640x640_(RetinaNet101)': {\n",
    "        'model_name': 'ssd_resnet101_v1_fpn_640x640_coco17_tpu-8',\n",
    "        'pipeline_file': 'pipeline.config',\n",
    "        'batch_size': 16\n",
    "    },\n",
    "    'SSD_MobileNet_V2_FPNLite_640x640': {\n",
    "        'model_name': 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8',\n",
    "        'pipeline_file': 'pipeline.config',\n",
    "        'batch_size': 8\n",
    "    }\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8O6jpdGAfwn"
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------- DOWNLOADING THE BASE MODEL -------------------------------------------------------\n",
    "# Changing the current work directory to the desired path\n",
    "# of the local computer in the jupyter notebook\n",
    "%cd $rep_path/WDS/models/research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ap3jTT8lAh-V"
   },
   "outputs": [],
   "source": [
    "# This is a tensorflow project so any configuration, libraries etc. mentioned come from that source.\n",
    "# Along with any URL generated and any models installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUw3w96MAj_i"
   },
   "outputs": [],
   "source": [
    "%cd $rep_path/WDS/models/research\n",
    "# Retrieving the model name for the selected model from a configuration dictionary called MODELS_CONFIG\n",
    "# using the selected_model variable as the key. \n",
    "# The retrieved model name is then stored in the MODEL variable.\n",
    "model = MODELS_CONFIG[selected_model]['model_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsu-K5VtAl5K"
   },
   "outputs": [],
   "source": [
    "%cd $rep_path/WDS/models/research\n",
    "# Retrieving the pipeline file name for the selected model from a configuration dictionary called MODELS_CONFIG\n",
    "# using the selected_model variable as the key. \n",
    "# The retrieved model name is then stored in pipe_line.\n",
    "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0hXU7AZAnlU"
   },
   "outputs": [],
   "source": [
    "%cd $rep_path/WDS/models/research\n",
    "# Retrieving the batch size for the selected model from a configuration dictionary called MODELS_CONFIG\n",
    "# using the selected_model variable as the key. \n",
    "# The retrieved model name is then stored in batch_size.\n",
    "batch_size = MODELS_CONFIG[selected_model]['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnsWazAvApft"
   },
   "outputs": [],
   "source": [
    "%cd $rep_path/WDS/models/research\n",
    "# Concatenating the model with it's file type to create the name of the model file to be downloaded.\n",
    "# The resulting filename is stored in the MODEL_FILE variable.\n",
    "model_file = model + '.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IxnnoWMBArUa"
   },
   "outputs": [],
   "source": [
    "%cd $rep_path/WDS/models/research\n",
    "# Setting the base URL for the download location of the selected model.\n",
    "download_base = 'http://download.tensorflow.org/models/object_detection/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#where model is saved\n",
    "dest_dir = rep_path+'WDS/pre-trained-models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5KjahZ0Atlf"
   },
   "outputs": [],
   "source": [
    "%cd $rep_path/WDS/models/research\n",
    "# Checking if the model_file already exists in the current working directory. \n",
    "# If it does not exist, it downloads the file from the URL download_base + model_file and saves it as model_file.\n",
    "if not (os.path.exists(model_file)):\n",
    "    urllib.request.urlretrieve(download_base + model_file, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_SYvLfwAvKU"
   },
   "outputs": [],
   "source": [
    "%cd $rep_path/WDS/models/research\n",
    "# Exctracting the downloaded model file by opening the 'file type' file \n",
    "# using the 'type file' module, extracting all of its contents, and then closing the file.\n",
    "# Unzipping the contents of the model\n",
    "# Written is an example of a tar file being extracted\n",
    "# Change to your needs\n",
    "tar = tarfile.open(model_file)\n",
    "tar.extractall()\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gOXvhg1bAzop"
   },
   "outputs": [],
   "source": [
    "%cd $rep_path/WDS/models/research\n",
    "# creating an output file to save the model while training\n",
    "# Removing the downloaded file type using the os.remove() function, \n",
    "# This removes any existing dest_dir directory using the shutil.rmtree() function, \n",
    "# and finally renames the extracted model directory to dest_dir. \n",
    "#This process ensures that the extracted model files are saved in the correct destination directory with the correct name.\n",
    "os.remove(model_file)\n",
    "if (os.path.exists(dest_dir)):\n",
    "    shutil.rmtree(dest_dir)\n",
    "os.rename(model, dest_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jn7eyGtjA2k7"
   },
   "outputs": [],
   "source": [
    "# Checking the content of the model\n",
    "%cd $rep_path/WDS/models/research\n",
    "\n",
    "!echo {dest_dir}\n",
    "!ls -alh {dest_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wW1wzFdUA4P_"
   },
   "outputs": [],
   "source": [
    "#the actual model file name\n",
    "fine_tune_checkpoint = os.path.join(dest_dir, \"model.ckpt\")\n",
    "fine_tune_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copying pipeline configuration file to seprate folder\n",
    "#allows use to create mutiple models with the same base\n",
    "%cd $rep_path/WDS\n",
    "!mkdir our_models\n",
    "%cd $rep_path/WDS/our_models\n",
    "!mkdir our_SSD_MobileNet_V2_FPNLite_640x640_B\n",
    "\n",
    "%cp  $rep_path/WDS/pre-trained-models/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/pipeline.config $rep_path/WDS/our_models/our_SSD_MobileNet_V2_FPNLite_640x640_B\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIGURING THE THE TRAINING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vxj0vAwA7UM"
   },
   "outputs": [],
   "source": [
    "#--------------------------------------- CONFIGURING THE THE TRAINING PIPELINE -------------------------------------------------\n",
    "\n",
    "\n",
    "# This part is configuring the training pipeline for a weapon detection system using the TensorFlow Object Detection API. \n",
    "# It is writing a configuration file for the SSD-MobileNet-V2 model, which is a popular object detection model.\n",
    "\n",
    "# The %%writefile command is a Jupyter notebook-specific command that writes the following code to a file. \n",
    "# In this case, the code is being written to the file \"file path\"\n",
    "\n",
    "# The code is divided into three main sections: model, train_config, and train_input_reader.\n",
    "\n",
    "# Model section:\n",
    "\n",
    "# The model section specifies the architecture of the model, including the type of object detection model (ssd), \n",
    "# the number of classes being detected (num_classes: 1), \n",
    "# the box coder used for encoding bounding box coordinates, \n",
    "# and the anchor generator used to generate anchor boxes for the model.\n",
    "\n",
    "\n",
    "# The image_resizer specifies the input image size for the model. \n",
    "# In this case, the input images will be resized to 300x300 pixels.\n",
    "# The box_predictor specifies the architecture of the box predictor network,\n",
    "# which predicts the class and location of each object in the image.\n",
    "# The feature_extractor specifies the architecture of the feature extractor network, \n",
    "# which extracts features from the input image.\n",
    "# The loss specifies the loss function used to train the model.\n",
    "\n",
    "\n",
    "# Train_config section:\n",
    "\n",
    "# The batch_size specifies the number of images used in each training batch.\n",
    "# The optimizer specifies the optimization algorithm used to train the model. \n",
    "# In this case, the model is trained using the RMSProp optimizer.\n",
    "# The fine_tune_checkpoint specifies the path to the pre-trained model checkpoint used for transfer learning.\n",
    "# The num_steps specifies the number of training steps to run.\n",
    "# The data_augmentation_options specify the data augmentation techniques used during training.\n",
    "\n",
    "\n",
    "\n",
    "# Train_input_reader section:\n",
    "\n",
    "# The tf_record_input_reader specifies the path to the TFRecords file containing the training data.\n",
    "# The label_map_path specifies the path to the label map file, which maps the class names to integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcjRc7A8BBm2"
   },
   "outputs": [],
   "source": [
    "%%writefile $rep_path/WDS/our_models/our_SSD_MobileNet_V2_FPNLite_320x320/pipeline.config\n",
    "model {\n",
    "  ssd {\n",
    "    num_classes: 2\n",
    "    image_resizer {\n",
    "      fixed_shape_resizer {\n",
    "        height: 320\n",
    "        width: 320\n",
    "      }\n",
    "    }\n",
    "    feature_extractor {\n",
    "      type: \"ssd_mobilenet_v2_fpn_keras\"\n",
    "      depth_multiplier: 1.0\n",
    "      min_depth: 16\n",
    "      conv_hyperparams {\n",
    "        regularizer {\n",
    "          l2_regularizer {\n",
    "            weight: 3.9999998989515007e-05\n",
    "          }\n",
    "        }\n",
    "        initializer {\n",
    "          random_normal_initializer {\n",
    "            mean: 0.0\n",
    "            stddev: 0.009999999776482582\n",
    "          }\n",
    "        }\n",
    "        activation: RELU_6\n",
    "        batch_norm {\n",
    "          decay: 0.996999979019165\n",
    "          scale: true\n",
    "          epsilon: 0.0010000000474974513\n",
    "        }\n",
    "      }\n",
    "      use_depthwise: true\n",
    "      override_base_feature_extractor_hyperparams: true\n",
    "      fpn {\n",
    "        min_level: 3\n",
    "        max_level: 7\n",
    "        additional_layer_depth: 128\n",
    "      }\n",
    "    }\n",
    "    box_coder {\n",
    "      faster_rcnn_box_coder {\n",
    "        y_scale: 10.0\n",
    "        x_scale: 10.0\n",
    "        height_scale: 5.0\n",
    "        width_scale: 5.0\n",
    "      }\n",
    "    }\n",
    "    matcher {\n",
    "      argmax_matcher {\n",
    "        matched_threshold: 0.5\n",
    "        unmatched_threshold: 0.5\n",
    "        ignore_thresholds: false\n",
    "        negatives_lower_than_unmatched: true\n",
    "        force_match_for_each_row: true\n",
    "        use_matmul_gather: true\n",
    "      }\n",
    "    }\n",
    "    similarity_calculator {\n",
    "      iou_similarity {\n",
    "      }\n",
    "    }\n",
    "    box_predictor {\n",
    "      weight_shared_convolutional_box_predictor {\n",
    "        conv_hyperparams {\n",
    "          regularizer {\n",
    "            l2_regularizer {\n",
    "              weight: 3.9999998989515007e-05\n",
    "            }\n",
    "          }\n",
    "          initializer {\n",
    "            random_normal_initializer {\n",
    "              mean: 0.0\n",
    "              stddev: 0.009999999776482582\n",
    "            }\n",
    "          }\n",
    "          activation: RELU_6\n",
    "          batch_norm {\n",
    "            decay: 0.996999979019165\n",
    "            scale: true\n",
    "            epsilon: 0.0010000000474974513\n",
    "          }\n",
    "        }\n",
    "        depth: 128\n",
    "        num_layers_before_predictor: 4\n",
    "        kernel_size: 3\n",
    "        class_prediction_bias_init: -4.599999904632568\n",
    "        share_prediction_tower: true\n",
    "        use_depthwise: true\n",
    "      }\n",
    "    }\n",
    "    anchor_generator {\n",
    "      multiscale_anchor_generator {\n",
    "        min_level: 3\n",
    "        max_level: 7\n",
    "        anchor_scale: 4.0\n",
    "        aspect_ratios: 1.0\n",
    "        aspect_ratios: 2.0\n",
    "        aspect_ratios: 0.5\n",
    "        scales_per_octave: 2\n",
    "      }\n",
    "    }\n",
    "    post_processing {\n",
    "      batch_non_max_suppression {\n",
    "        score_threshold: 9.99999993922529e-09\n",
    "        iou_threshold: 0.6000000238418579\n",
    "        max_detections_per_class: 100\n",
    "        max_total_detections: 100\n",
    "        use_static_shapes: false\n",
    "      }\n",
    "      score_converter: SIGMOID\n",
    "    }\n",
    "    normalize_loss_by_num_matches: true\n",
    "    loss {\n",
    "      localization_loss {\n",
    "        weighted_smooth_l1 {\n",
    "        }\n",
    "      }\n",
    "      classification_loss {\n",
    "        weighted_sigmoid_focal {\n",
    "          gamma: 2.0\n",
    "          alpha: 0.25\n",
    "        }\n",
    "      }\n",
    "      classification_weight: 1.0\n",
    "      localization_weight: 1.0\n",
    "    }\n",
    "    encode_background_as_zeros: true\n",
    "    normalize_loc_loss_by_codesize: true\n",
    "    inplace_batchnorm_update: true\n",
    "    freeze_batchnorm: false\n",
    "  }\n",
    "}\n",
    "train_config {\n",
    "  batch_size: 16\n",
    "  data_augmentation_options {\n",
    "    random_horizontal_flip {\n",
    "    }\n",
    "  }\n",
    "  data_augmentation_options {\n",
    "    random_crop_image {\n",
    "      min_object_covered: 0.0\n",
    "      min_aspect_ratio: 0.75\n",
    "      max_aspect_ratio: 3.0\n",
    "      min_area: 0.75\n",
    "      max_area: 1.0\n",
    "      overlap_thresh: 0.0\n",
    "    }\n",
    "  }\n",
    "  sync_replicas: true\n",
    "  optimizer {\n",
    "    momentum_optimizer {\n",
    "      learning_rate {\n",
    "        cosine_decay_learning_rate {\n",
    "          learning_rate_base: 0.07999999821186066\n",
    "          total_steps: 50000\n",
    "          warmup_learning_rate: 0.026666000485420227\n",
    "          warmup_steps: 1000\n",
    "        }\n",
    "      }\n",
    "      momentum_optimizer_value: 0.8999999761581421\n",
    "    }\n",
    "    use_moving_average: false\n",
    "  }\n",
    "  fine_tune_checkpoint: \"/Users/Aida/Documents/GitHub/Rep2/WDS/pre-trained-models/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0\"\n",
    "  num_steps: 50000\n",
    "  startup_delay_steps: 0.0\n",
    "  replicas_to_aggregate: 8\n",
    "  max_number_of_boxes: 100\n",
    "  unpad_groundtruth_tensors: false\n",
    "  fine_tune_checkpoint_type: \"detection\"\n",
    "  fine_tune_checkpoint_version: V2\n",
    "}\n",
    "train_input_reader {\n",
    "  label_map_path: \"/Users/Aida/Documents/GitHub/Rep2/WDS/data/label_map.pbtxt\"\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"/Users/Aida/Documents/GitHub/Rep2/WDS/data/train_labels.record\"\n",
    "  }\n",
    "}\n",
    "eval_config {\n",
    "  metrics_set: \"coco_detection_metrics\"\n",
    "  use_moving_averages: false\n",
    "}\n",
    "eval_input_reader {\n",
    "  label_map_path: \"/Users/Aida/Documents/GitHub/Rep2/WDS/data/label_map.pbtxt\"\n",
    "  shuffle: false\n",
    "  num_epochs: 1\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"/Users/Aida/Documents/GitHub/Rep2/WDS/data/test_labels.record\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $rep_path/WDS/our_models/our_SSD_MobileNet_V2_FPNLite_640x640_B/pipeline.config\n",
    "model {\n",
    "  ssd {\n",
    "    num_classes: 2\n",
    "    image_resizer {\n",
    "      fixed_shape_resizer {\n",
    "        height: 640\n",
    "        width: 640\n",
    "      }\n",
    "    }\n",
    "    feature_extractor {\n",
    "      type: \"ssd_mobilenet_v2_fpn_keras\"\n",
    "      depth_multiplier: 1.0\n",
    "      min_depth: 16\n",
    "      conv_hyperparams {\n",
    "        regularizer {\n",
    "          l2_regularizer {\n",
    "            weight: 3.9999998989515007e-05\n",
    "          }\n",
    "        }\n",
    "        initializer {\n",
    "          random_normal_initializer {\n",
    "            mean: 0.0\n",
    "            stddev: 0.009999999776482582\n",
    "          }\n",
    "        }\n",
    "        activation: RELU_6\n",
    "        batch_norm {\n",
    "          decay: 0.996999979019165\n",
    "          scale: true\n",
    "          epsilon: 0.0010000000474974513\n",
    "        }\n",
    "      }\n",
    "      use_depthwise: true\n",
    "      override_base_feature_extractor_hyperparams: true\n",
    "      fpn {\n",
    "        min_level: 3\n",
    "        max_level: 7\n",
    "        additional_layer_depth: 128\n",
    "      }\n",
    "    }\n",
    "    box_coder {\n",
    "      faster_rcnn_box_coder {\n",
    "        y_scale: 10.0\n",
    "        x_scale: 10.0\n",
    "        height_scale: 5.0\n",
    "        width_scale: 5.0\n",
    "      }\n",
    "    }\n",
    "    matcher {\n",
    "      argmax_matcher {\n",
    "        matched_threshold: 0.5\n",
    "        unmatched_threshold: 0.5\n",
    "        ignore_thresholds: false\n",
    "        negatives_lower_than_unmatched: true\n",
    "        force_match_for_each_row: true\n",
    "        use_matmul_gather: true\n",
    "      }\n",
    "    }\n",
    "    similarity_calculator {\n",
    "      iou_similarity {\n",
    "      }\n",
    "    }\n",
    "    box_predictor {\n",
    "      weight_shared_convolutional_box_predictor {\n",
    "        conv_hyperparams {\n",
    "          regularizer {\n",
    "            l2_regularizer {\n",
    "              weight: 3.9999998989515007e-05\n",
    "            }\n",
    "          }\n",
    "          initializer {\n",
    "            random_normal_initializer {\n",
    "              mean: 0.0\n",
    "              stddev: 0.009999999776482582\n",
    "            }\n",
    "          }\n",
    "          activation: RELU_6\n",
    "          batch_norm {\n",
    "            decay: 0.996999979019165\n",
    "            scale: true\n",
    "            epsilon: 0.0010000000474974513\n",
    "          }\n",
    "        }\n",
    "        depth: 128\n",
    "        num_layers_before_predictor: 4\n",
    "        kernel_size: 3\n",
    "        class_prediction_bias_init: -4.599999904632568\n",
    "        share_prediction_tower: true\n",
    "        use_depthwise: true\n",
    "      }\n",
    "    }\n",
    "    anchor_generator {\n",
    "      multiscale_anchor_generator {\n",
    "        min_level: 3\n",
    "        max_level: 7\n",
    "        anchor_scale: 4.0\n",
    "        aspect_ratios: 1.0\n",
    "        aspect_ratios: 2.0\n",
    "        aspect_ratios: 0.5\n",
    "        scales_per_octave: 2\n",
    "      }\n",
    "    }\n",
    "    post_processing {\n",
    "      batch_non_max_suppression {\n",
    "        score_threshold: 9.99999993922529e-09\n",
    "        iou_threshold: 0.6000000238418579\n",
    "        max_detections_per_class: 100\n",
    "        max_total_detections: 100\n",
    "        use_static_shapes: false\n",
    "      }\n",
    "      score_converter: SIGMOID\n",
    "    }\n",
    "    normalize_loss_by_num_matches: true\n",
    "    loss {\n",
    "      localization_loss {\n",
    "        weighted_smooth_l1 {\n",
    "        }\n",
    "      }\n",
    "      classification_loss {\n",
    "        weighted_sigmoid_focal {\n",
    "          gamma: 2.0\n",
    "          alpha: 0.25\n",
    "        }\n",
    "      }\n",
    "      classification_weight: 1.0\n",
    "      localization_weight: 1.0\n",
    "    }\n",
    "    encode_background_as_zeros: true\n",
    "    normalize_loc_loss_by_codesize: true\n",
    "    inplace_batchnorm_update: true\n",
    "    freeze_batchnorm: false\n",
    "  }\n",
    "}\n",
    "train_config {\n",
    "  batch_size: 8\n",
    "  data_augmentation_options {\n",
    "    random_horizontal_flip {\n",
    "    }\n",
    "  }\n",
    "  data_augmentation_options {\n",
    "    random_crop_image {\n",
    "      min_object_covered: 0.0\n",
    "      min_aspect_ratio: 0.75\n",
    "      max_aspect_ratio: 3.0\n",
    "      min_area: 0.75\n",
    "      max_area: 1.0\n",
    "      overlap_thresh: 0.0\n",
    "    }\n",
    "  }\n",
    "  sync_replicas: true\n",
    "  optimizer {\n",
    "    momentum_optimizer {\n",
    "      learning_rate {\n",
    "        cosine_decay_learning_rate {\n",
    "          learning_rate_base: 0.07999999821186066\n",
    "          total_steps: 50000\n",
    "          warmup_learning_rate: 0.026666000485420227\n",
    "          warmup_steps: 1000\n",
    "        }\n",
    "      }\n",
    "      momentum_optimizer_value: 0.8999999761581421\n",
    "    }\n",
    "    use_moving_average: false\n",
    "  }\n",
    "  fine_tune_checkpoint: \"/Users/Aida/Documents/GitHub/Rep2/WDS/pre-trained-models/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/checkpoint/ckpt-0\"\n",
    "  num_steps: 50000\n",
    "  startup_delay_steps: 0.0\n",
    "  replicas_to_aggregate: 8\n",
    "  max_number_of_boxes: 100\n",
    "  unpad_groundtruth_tensors: false\n",
    "  fine_tune_checkpoint_type: \"detection\"\n",
    "  fine_tune_checkpoint_version: V2\n",
    "}\n",
    "train_input_reader {\n",
    "  label_map_path: \"/Users/Aida/Documents/GitHub/Rep2/WDS/data/label_map.pbtxt\"\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"/Users/Aida/Documents/GitHub/Rep2/WDS/data/train_labels.record\"\n",
    "  }\n",
    "}\n",
    "eval_config {\n",
    "  metrics_set: \"coco_detection_metrics\"\n",
    "  use_moving_averages: false\n",
    "}\n",
    "eval_input_reader {\n",
    "  label_map_path: \"/Users/Aida/Documents/GitHub/Rep2/WDS/data/label_map.pbtxt\"\n",
    "  shuffle: false\n",
    "  num_epochs: 1\n",
    "  tf_record_input_reader {\n",
    "    input_path: \"/Users/Aida/Documents/GitHub/Rep2/WDS/data/test_labels.record\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to verify write\n",
    "pipeline_fname=rep_path+'/WDS/our_models/our_SSD_MobileNet_V2_FPNLite_640x640_B/pipeline.config'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEvDl36zBChw"
   },
   "outputs": [],
   "source": [
    "  # Checking the config file after editing \n",
    "!cat {pipeline_fname} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5Dtsf6VErc1"
   },
   "source": [
    "tensor board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor board is used to track model while training\n",
    "%pip install -U tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strting tensor board\n",
    "#run in ternimal parelle to traning to see progress hwile training\n",
    "%cd $rep_path/WDS\n",
    "model_dir=rep_path+'/WDS/our_models/our_SSD_MobileNet_V2_FPNLite_640x640_B'\n",
    "!tensorboard --logdir=model_dir\n",
    "\n",
    "#For upload to website to view remotly\n",
    "!tensorboard dev upload --logdir \\\n",
    "    'model_dir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining model directory\n",
    "model_dir=rep_path+'/WDS/our_models/our_SSD_MobileNet_V2_FPNLite_640x640_B'\n",
    "model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining model directory\n",
    "\n",
    "pipeline_config_path=rep_path+'/WDS/our_models/our_SSD_MobileNet_V2_FPNLite_640x640_B/pipeline.config'\n",
    "pipeline_config_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd $rep_path/WDS/models/research\n",
    "#start training\n",
    "!python3 /Users/Aida/Documents/GitHub/Rep2/WDS/models/research/object_detection/model_main_tf2.py  \\\n",
    "    --pipeline_config_path={pipeline_config_path} \\\n",
    "    --model_dir={model_dir} \\\n",
    "    --alsologtostderr \\\n",
    "    --num_train_steps={num_steps} \\\n",
    "    --num_eval_steps={num_eval_steps}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependancy of cocoapi\n",
    "%pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run in terminal while training to evaluate over time\n",
    "#using cocoapi metrics\n",
    "%cd $rep_path/WDS\n",
    "!python $rep_path/WDS/models/research/object_detection/model_main_tf2.py \\\n",
    "    --model_dir={model_dir}\\\n",
    "    --pipeline_config_path={pipeline_config_path} \\\n",
    "    --checkpoint_dir={model_dir}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporting model to compact useable format \n",
    "%cd $rep_path/WDS\n",
    "!python $rep_path/WDS/exporter_main_v2.py \\\n",
    "    --input_type image_tensor \\\n",
    "    --pipeline_config_path $rep_path/WDS/our_models/our_SSD_MobileNet_V2_FPNLite_640x640_B/pipeline.config \\\n",
    "    --trained_checkpoint_dir $rep_path/WDS/our_models/our_SSD_MobileNet_V2_FPNLite_640x640_B \\\n",
    "    --output_directory $rep_path/WDS/our_exported-models/our_model_4\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
